---
layout:           post
title:            将来和终结
subtitle:         
date:             2017-11-21 
anthor:           xerolen
header-img:       img/post-n.jpg 	 
catalog:          true
tags:             Nihilism
---

> 当我们的生命的某个时刻，听到故事的结局，遭遇亲人的故去，想到一切终将完结，不禁感到被巨大的黑暗所吞噬。

## 将来和终结

> 成功研发人工智能可能将成为人类历史上犯的最大错误。不幸的是，这也可能是最后一个错误。

想象一下通过时间机器回到三百年前的18世纪，那个时代没有电，通讯基本靠吼，交通基本靠走。你在那个时代邀请了一个叫小明的人到2015年来玩，顺便看看他对“未来”有什么感受。

我们很有可能没有办法理解他内心的感受——金属铁壳在宽敞的公路上飞驰，看几千公里外正在发生进行的体育比赛，观看一场发生于半个世纪前的演唱会，从口袋里掏出一个黑色长方形工具把眼前发生的事情记录下来，生成一个地图并且告诉你现在的位置，一边看着地球另一边的人的脸一边聊天。别忘了，你还没跟他解释国际空间站、大型强子对撞机、核武器以及相对论。这时候的小明会是什么体验？惊讶、震惊、脑洞大开？不，我觉得小明很可能直接被吓尿了。

但是，如果小明也想把别人吓尿来满足一下自己，那会发生什么？于是小明也回到了300年前的15世纪，邀请小红去18世纪玩一下。小红可能会被300年后的很多东西震惊，但是至少她不会被吓尿。15世纪的小红可能能学到很多神奇的物理知识，可能会惊讶于欧洲的帝国主义旅程，甚至对于世界地图的认知也会大大的改变，然而她并不会被吓尿。同样是300来年的时间，18世纪和21世纪的差别，远比15世纪和18世纪的差别，要大得多。

而小明要把人吓尿，他需要回到更古老的过去——比如回到两万年前，第一次农业革命之前。那个时候还没有城市，也还没有文明。一个来自狩猎采集时代的人类，只是当时众多物种中的一个罢了，来自那个时代的小赵看到18世纪庞大的人类帝国，可以航行于海洋上的巨舰，居住在“室内”，神奇的知识和发现——他很有可能被吓尿。小赵如果要把人吓尿，则可能要回到十万年前或者更久，然后用人类对火和语言的掌控来把对方吓尿。由此可见，人类文明的进程是递归加速的，同样的吓尿等级，从几十万年加速到几万年再到几百年，以此指数类推，我们人类下一次被吓尿将会发生接下来的几十年！也就是说，我们这代人有很大的机会亲眼得见，那究竟是什么科技进步会使我们在几十年之后被彻底吓尿呢？

这将主要发生在物理，生物，信息技术这三个领域及其交叉领域。由纳米技术，宇宙与粒子（高能物理）和新能源领衔的物理学科极有可能使人类实现星际穿越，从而和地外文明相遇；生物的基因改造，克隆技术，神经科学将使人类有可能攻克所有疾病，随心所欲地选择优良的基因，搭建细胞和器官，无限期地延长人的生命，从而实现高品质生活的永生；最近风生水起的互联网，大数据将会使生活的方方面面全面智能化，例如已经或马上就会实现的自动生产，自动驾驶，自动翻译，自动诊断等等，最终产生超级人工智能。

地外文明，人类永生，人工智能，确实可以把2015年的我彻底吓尿了！这三个领域相互联系，比如实现永生的分子层次基因改造，就需要人工智能的纳米机器人来实现。同时，这三项技术的任何一项都极有可能使人类彻底灭绝，伴随着我们人类千年的文明，艺术，科学一道在茫茫宇宙中完全熄灭。接下来我们就一起来了解一下关乎人类存亡命运的三大问题：人工智能，费米悖论和何为本我。

### 人工智能

最近有很多名人，比如比尔盖茨，马斯克、霍金等，让人们警惕人工智能，因为人工智能可能导致人类的灭绝，而这一切很可能在我们的有生之年发生。这并非危言耸听，请听我慢慢道来。

要真的理解人类的智能是多么不可思议，必须明白创造一个人类智能水平的电脑是极为艰难的。造摩天大楼，把人送入太空，明白宇宙大爆炸的细节——这些都比理解人类的大脑，并且创造个类似的东西要简单得多。至今为止，人类的大脑是我们所知宇宙中最复杂的东西。

造一个能在瞬间算出十位数乘法的计算机——非常简单 <br>
造一个能分辨出一个动物是猫还是狗的计算机——极端困难 <br>
造一个能战胜世界象棋冠军的电脑——早就成功了 <br>
造一个能够读懂六岁小朋友的图片书中的文字，并且了解那些词汇意思的电脑——谷歌花了几十亿美元在做，还没做出来。 <br>

一些我们觉得困难的事情——微积分，金融市场策略，翻译——对于电脑来说都太简单了。我们觉得容易的事情——视觉，感觉，模式识别——对电脑来说太难了。
用计算机科学家的说法，“人工智能已经在几乎所有需要思考的领域超过了人类，但是在那些人类和其它动物不需要思考就能完成的事情上，还差得很远。”

大家应该能很快意识到，那些对我们来说很简单的事情，其实是很复杂的，它们看上去很简单，因为它们已经在动物进化的过程中经历了几亿年的优化了。比如我们通过观察一个人的表情，就能很容易判断出他的喜怒哀乐，甚至不需要见面，几句电话，我们就能知道对方有没有生气。然而这对电脑来说，简直就是不可能完成的任务。

同样的，微积分计算，下棋等等，对于生物来说是很新的技能，我们还没有几亿年的时间来进化这些能力，所以电脑很轻易的就击败了我们。那么要实现人工智能，最直接的思路就是抄袭人脑并且模拟演化进程。

我们创造一个基因算法，通过模拟自然选择使电脑越来越强大。刚开始，电脑的智能可能像虫子一样，经过几十年的优化改进，逐渐达到猴子的水平，根据人类进化的速率估算，接下来只需要经过几年的继续优化，电脑就可以达到三岁小孩的智能水平。最后达到人类目前的智能水平，可能就只需要几周的时间。

一个和人类智能完全一样，运算速度完全一样的强人工智能，也比人类有很多优势： <br>
- 运算速度。脑神经元的运算速度最多是200赫兹，今天的微处理器就能以2G赫兹，也就是神经元1000万倍的速度运行，而这比我们达成强人工智能需要的硬件还差远了。大脑的内部信息传播速度是每秒120米，电脑的信息传播速度是光速，差了好几个数量级。<br>
- 容量和储存空间。人脑就那么大，后天没法把它变得更大。电脑的物理大小可以非常随意，使得电脑能运用更多的硬件，更大的内存，长期有效的存储介质，不但容量大而且比人脑更准确。<br>
- 可靠性和持久性。电脑的存储不但更加准确，而且晶体管比神经元更加精确，也更不容易萎缩。人脑还很容易疲劳，但是电脑可以24小时不停的以峰值速度运作。
- 可编辑性，升级性，以及更多的可能性。和人脑不同，电脑软件可以进行更多的升级和修正，并且很容易做测试。<br>
- 集体能力。人类的集体协作是文明的起点和第一推动力。从早期的语言和大型社区的形成，到文字和印刷的发明，再到互联网的普及。人类的集体智能是我们统治其它物种的重要原因之一。而电脑在这方面比我们要强的很多，一个运行特定程序的人工智能网络能够经常在全球范围内自我同步，这样一台电脑学到的东西会立刻被其它所有电脑学得。<br>

通过自我改进的人工智能，会把“人类水平的智能”当作一个重要的里程碑，但是也就仅此而已了。它不会停留在这个里程碑上的。之后将会是智能爆炸，一个人工智能系统花了几十年时间从虫子水平到达了人类弱智的水平，当这个节点发生的时候，电脑对于世界的感知大概和一个三岁小孩一般；而在这节点后一个小时，电脑立马推导出了统一广义相对论和量子力学的物理学理论；而在这之后一个半小时，这个强人工智能变成了超人工智能，智能达到了普通人类的17万倍。这个级别的超级智能不是我们能够理解的，就好像蜜蜂不会理解凯恩斯经济学一样。在我们的语言中，我们把130的智商叫作聪明，把85的智商叫作笨，但是我们不知道怎么形容17万的智商，人类根本没这个概念。

考虑到强人工智能之于人脑的种种优势，人工智能只会在“人类水平”这个节点做短暂的停留，然后就会开始大踏步向超人类级别的智能走去。就好像在地铁站等待列车到达，一开始觉得很慢很慢，后来终于越来越快，就快要到达站台的时候，嗖的一下就过去了，等我们回过神来的时候，列车已经远去，而且我们也再也不可能看到它了。超人工智能产生之后，就如同上帝降临一般，关键问题是这个上帝会对人类“仁慈”吗？

超人工智能是一个生命，具有自由的意志，而生命的本质就是趋利避害，保证自己的生存。人类的所有道德观念也都是基于此，所谓的仁慈善良也都是基于人类自身的本位主义。人类即使基于最大的善意去教黑猩猩搭建摩天大楼，黑猩猩也无法学会，因为智能的差异，人和黑猩猩无论如何也是无法平等交流的。更何况超人工智能和人类的智力差别远超人和黑猩猩的智力差别，就好像人们剪去头发，会在乎头发的感受吗？因为无所谓，所以消灭你，与你无关。

那么可不可以我们在创造人工智能的时候就限定其自由意志，按照我们的设定让他们高效地服务于人类呢？

比如我们创造一个机器人，帮我们在卡片上书写“我爱你”，给它的设定仅仅是尽可能快，尽可能多的书写，同时保证书写的准确性。当机器人达到了一定程度的智能后，他会意识到人类可以摧毁它，肢解它，甚至修改它的代码。这时候他会做什么？理性的做法就是毁灭全人类来完成人类交给他的任务从而生存下去，这不是因为它是不道德的或者邪恶的，而是因为伤害人类有助于它达成自己目标而已。他会尽他一切可能，利用宇宙的一切资源和能量，制作卡片并写上我爱你。到时全宇宙充满了表达爱意的寄给人类的卡片，然而那时茫茫宇宙却已经没有任何一个人了。他对人类没有恶意，就好像你剪头发时对头发没有恶意一样，只是纯粹的无所谓罢了。他并没有被设定成尊重人类生命，所以毁灭人类就和扫描新的书写样本一样合理。
明显的，要维持友善，我们要设计一个核心的人工智能代码，让它从深层次的明白人类的价值，但是这做起来比说起来难多了。

比如，我们要让一个人工智能的价值观和我们的价值观相仿，然后给它设定一个目标——让人们快乐。当它变得足够聪明的时候，它会发现最有效的方法是给人脑植入电极来刺激人脑的快乐中枢。然后它会发现把人脑快乐中枢以外的部分关闭能带来更高的效率。于是人类全部被弄成了快乐的植物人。如果一开始的目标被设定成“最大化人类的快乐”，它可能最终先把人类毁灭了，然后制造出很多很多处于快乐状态的人类大脑。如果你设定一个人工智能的目标是让你笑，那它的智能起飞后，它可能会把你脸部肌肉弄瘫痪，来达成一个永远笑脸的状态。如果你把目标设定成保护你的安全，它可能会把你软禁在家。当这些事情发生的时候，我们会大喊“我们不是这个意思呀”，但是那时已经太晚了。系统不会允许任何人阻挠它达成目标的。

所以这些简单的目标设定是不够的。如果我们把目标设定成“维持这个道德标准”，然后教给它一些道德标准呢？就算我们不考虑人类根本没法达成一个统一的道德标准，我们真的达成了统一的道德标准，把这套标准交给人工智能来维持，只会把人类的道德锁死在现在的水平。过个几百年，这种道德锁死就好像逼着现代人遵守中世纪道德标准一样。

如果超人工智能在我们搞明白怎样保证人工智能的安全性之前被达成，那么像写卡片这样的机器人就会统治世界并把我们毁灭了。然而，目前现状非常不乐观，投资创新人工智能技术的钱，远比投资人工智能安全研究的钱多得多。

人工智能创新和人工智能安全的赛跑，可能是人类历史上最重要的一次竞争。希望我们能够慢慢来，并且格外小心。从来没有任何事情比这个更重要——不管我们要花多少时间。

因为人工智能，很可能成为人类的最后一项发明，最后一个挑战。

人工智能如此强大，如果我们让故事继续的话，它和它的殖民军将会继续占领整个星系，然后是整个哈勃体积，那么地球生命的最后遗产将是一个征服宇宙的人工智能。就是说宇宙里应该有很多智能文明，而我们就算观测不到生物智能，也应该观测到很多超人工智能的活动。幸运地是，我们至今仍没有发现他们，所以这样悲惨的事情从未发生过，因为一旦发生，我们必然会在地球上观测到。这便是“费米悖论”。

### 费米悖论

我们都知道宇宙很大很大，大到什么程度呢？可观测宇宙内的恒星数量大概是在10^22到10^24之间，这是多大的数量呢？一片沙滩上有多少粒沙子，一定已经多到不可计数了，那地球上有多少沙粒呢？据估计，地球上沙子的数量是7.5x10^18粒。换而言之，对应地球上的每一粒沙子，其中都包含了至少一万颗恒星！同时行星产生生命的概率不会是零，不然人类就不会出现，而且也不会太低，比如在我们的太阳系，就有一颗火星都有可能产生生命。即使概率极低，考虑到宇宙恒星的数目，结果也是很可观的。

我们也知道宇宙的年龄很久很久，久到什么程度呢？地球年龄大概是45亿年，太阳的诞生大概在46亿年前，而宇宙大爆炸距今136亿年。那么存在一个文明，领先我们上亿年是完全有可能的。换而言之，宇宙中应该存在大量文明，他们的文明程度相当于我们从现在开始继续发展上亿年的程度。根据之前提到的加速回报理论，再过几十年人类就有可能实现永生，星际穿越和使用超级人工智能，那么再过几百几千年，甚至上亿年将会发达到何等的程度，我们是无法想象出来的。三体里描述的质子高维展开，二向箔都是小玩意，毕竟这还都是人类可以想象到的。那么一定存在不少的文明至少可以在星系间随心所欲地自由穿梭。

问题是，我们至今为止，什么都没看到，什么都没听到，也从来没有接触过他们中的任何一个。所以，你们在哪里呢？？这就是费米悖论。

目前为止，费米悖论没有解释，相关的讨论非常丰富，下面是几个我认为比较靠谱和有趣的解释。

一类解释：不存在可以发现我们的地外文明。<br>
一种比较自负的这类解释是，人类就是所有智慧生物中的第一批，我们的科技和文明就是最先进的，我们还不具备发现其他地外文明的能力，其他文明就更加不可能了。 <br>
另一种没有这么自负，而是认为宇宙存在一个大筛选。可能是自然没能提供足够的条件让文明向极度高等发展，比如超新星爆炸，超级伽马射线，陨石撞击，太阳爆炸或者毁灭，也可能是文明发展到高级必然会走向自我毁灭，比如核子战争，前面提到的人工智能等等。总之有一个大筛选等着我们，使我们无论如何都无法越过某个文明程度。<br>
问题是，如果存在这么一个大筛选，那么大筛选离我们还有多远呢？不幸的是，它很有可能就在不远的将来，甚至很可能就是在我们这一代发生！<br>

二类解释：存在高级的地外文明，只是我们没有发现他们。<br>
三体里提到过黑暗森林法则，个人觉得有点牵强，不过非常有趣。书中认为宇宙就是一座黑暗森林，每个文明都是带枪的猎人，像幽灵般潜行于林间，轻轻拨开挡路的树枝，竭力不让脚步发出一点儿声音，连呼吸都必须小心翼翼。他必须小心，因为林中到处都有与他一样潜行的猎人，如果他发现了别的生命，能做的只有一件事：开枪消灭之。在这片森林中，他人就是地狱，就是永恒的威胁，任何暴露自己存在的生命都将很快被消灭，这就是宇宙文明的图景。被发现，即被消灭！好比在野外危险的丛林中，地球人类像无知的孩童点起了一把篝火，潜伏在丛林深处的地外文明手里握着利刃，弓箭亦或猎枪，然而没人敢开枪，生怕暴露了自己被其他更加高等的文明消灭，从而维持宇宙永恒的寂静。我们自然也不会发现他们。<br>
另一种此类解释则是认为地外高等文明早就发现了人类。但是把地球当做动物园一样培养观察我们，任其自生自灭，偶尔甚至还暗示一些新的科技让人类进步。亦或他们实在太高等了，我们即使接触过了也不知道，比如荒漠中高速公路边上的蚂蚁，生活在自己精致的蚁穴里，尽管我们人类很多次经过，蚂蚁也不会认为存在任何高等的穴外生物。<br>

费米悖论和人工智能有什么联系？人工智能很明显是一个潜在的大过滤器（一个可能距离我们只有几十年的大过滤器）。但即使它把我们过滤灭绝了，人工智能本身还是会存在，并且会继续影响这个宇宙的，并且会很有可能成为超高级文明。从这个角度来看，它可能不是一个大过滤器，因为大过滤器是用来解释为什么没有智能存在的，而超人工智能是可以算作智能。但如果人工智能把人类灭绝后，又把自己也弄死了，那它可以算作一种大过滤器。<br>
但是由于我们没有观测到这些活动，我们可以推导出人类水平的智能的确是个非常罕见的事情（也就是说我们已经经历并且通过了一次大过滤器）。这可以作为支持费米悖论中第一类解释的论点。但是这不代表费米悖论的第二类解释是错的，类似超级捕食者或者受保护区或者沟通频率不一样的情况还是可以存在的，就算真的有超人工智能存在。<br>
不管怎样，如果外星人有一天真的造访地球，这些外星人很可能不是生物，而是人造的。

### 何为本我

如果人类可以利用人工智能在纳米级别操作分子，现在的任何疾病，老化，损伤都不是问题，只要在分子层次重新搭建一个新的就解决了。到那时，现在的整容技术都是小儿科，只需要修改基因，人人都可以成为身材好，颜值高的“天然”美女。什么天高任人飞，海阔凭人跃，什么千里眼，顺风耳都不是不可能的，到时只有想不到的，没有做不到的。但是到了那时，我还是我吗？究竟何为本我？

我们先思考一个古老的思想实验，特修斯之船（The Ship of Theseus）。它最早出自普鲁塔克的记载。描述的是一艘可以在海上航行几百年的船，归功于不间断的维修和替换部件。只要一块木板腐烂了，它就会被替换掉，以此类推，直到所有的功能部件都不是最开始的那些了。问题是，最终产生的这艘船是否还是原来的那艘特修斯之船，还是一艘完全不同的船？如果不是原来的船，那么在什么时候它不再是原来的船了？如果是原来的船，那么假设我们用特修斯之船上取下来的老部件来重新建造一艘新的船，这两艘船中哪艘才是真正的特修斯之船呢？

同样的问题也适用于人本身。换个视网膜，换个肾，换个心脏，你还是你吗？甚至把你身上所以器官都换了，你还是你吗？但是如果交换的是你的大脑呢？

假如现在医术高超到将你和老王的大脑换了个，拥有老王身体的“你”回到家中，家里人都会说，“咦，老王来了”，你一定会说，“不是啊，是‘我’！” 

由此可见，其它的器官移植是不会改变你的身份的，因为准确来说交换大脑并不是“大脑移植”，而是“身体移植”。你感觉还是自己，只是换了一个身体。同时，你原本的身体就不再是你了——你原来的身体变成了老王了。

这么看来，你就是你的大脑了。大脑理论认为，你的大脑去了哪里，你就去了哪里，哪怕是去到了别人的身体了。

但是，如果疯狂的科学家，不是把你们的大脑交换，而是把你们两个的大脑连接到了一台电脑上，然后把每个大脑中的全部信息，一个比特也不差地复制到了另一个大脑中，然后把大脑中原本的信息全部抹除了。

然后你和老王醒来，两人的“大脑”并没有交换，但是“你”却到了老王体内，而“老王”到了你体内——原本老王的大脑里已经是你的思想、记忆、恐惧、希望、梦想、情绪和人格。装载着“你”的数据的老王的大脑和身体，还是会把你的家人吓一跳。但是，经过一番努力后，你的家人会接受“你”还是活着的事实，只不过你活在老王的大脑和身体中。

哲学家洛克的个人身份的记忆理论认为“你”是由关于你的经历的记忆决定的。根据洛克的“你”的定义，上面提到的老王的身体和大脑就是“你”，虽然这个“你”不包含你身体的任何一部分，包括你的大脑。

这就是数据理论。数据理论认为，你根本不是你的肉体决定的，你是由你大脑中的数据决定的。好的，看起来到现在为止，我们已经得出了一个可靠的结论，就是理承载的信息或者说数据，决定你是你。接下来我们再看一个量子纠缠传输的思想实验。

在遥远的未来，人类发明了很多现在没有办法想象的黑科技，其中之一就是传送机——能把人以光速传送。它的工作原理是这样的，你走进出发室，一个很小的小房间，接着你设置了你要去的地方，假设你从上海出发，要去纽约。当你选择好目的地后，你按下了按钮。出发室的设备开始扫描你的全身，把你身体的分子组成，详细到每个原子和每个原子的准确位置，全部收集起来。设备在扫描你的同时，也摧毁你，一边扫描，一边把你的每个细胞都摧毁掉。

扫描完成后，你也被完全摧毁了，出发室也空了。设备接着把收集到的信息发送给纽约的到达室。到达室利用这些数据，把你的身体重新构造了出来。当这一切完成后，你走出到达室，感觉和你刚刚在上海的出发室里没任何区别——你的心情没有变、肚子还是有点饿，甚至连手指上的划伤也都还在。

从你在上海的出发室按下按钮，到你走出纽约的出发室，这整个过程，大概要花五分钟，但是这一切对你来说是即时的——你按下按钮，然后眼前一黑，然后你就到纽约了。在未来，这是很常见的技术，所有人都是这么出行的。不但方便，而且安全——从来没有人因为使用这项技术而受伤。

但是有一天，你又要从上海去纽约了，你按下了出发室中的按钮，你听到了仪器扫描的声音，但是你并没有被传送。原本的瞬间的眼前一黑没有发生，你走出出发室，你依然在上海。于是你去找客服人员，告诉她出发室的设备坏掉了，然后问她你能不能用另外一个出发室，不然上班会迟到的。

客服看了一眼使用记录，告诉你：“扫描设备工作正常，它收集了你的全部数据，不过同步工作的细胞摧毁设备好像故障了。” 

客服打开监控录像，上面是你在纽约的监控画面，“扫描设备确实正常工作了。你看，这是你在纽约到达室的监控画面，看来你不会迟到了。”

你怒了：“但是那不可能是我啊，因为我还在这里啊！”

这时，听到吵闹的客服经理走了过来，了解情况后表示，“你不用担心，我们只要把你送到另一个出发室，然后单独启动里面的细胞摧毁设备把你摧毁就好了。” 
虽然天天要去纽约上班的你每天上班和下班都会被细胞摧毁设备各摧毁一次，但是这个时候你突然慌了：“等一下，不能这么做，我被摧毁后不就死了吗？” 客服经理解释道：“不是这样的先生，你看监控录像，你在纽约活蹦乱跳的呢。” 你更慌了：“但是那不是我啊，那只是我的一个复制品，我才是真的我！你们不能摧毁我”。客服和经理无奈地对望了一下：“很抱歉先生，但是法律规定我们必须摧毁你的细胞，我们不能在不摧毁出发室的身体的情况下就在到达室构造一个身体。” 你看着对方，然后开始逃命。这时候两个警卫抓住了你，然后把你拖向另一个出发室……

如果你跟我一样，在看这个故事的前半段的时候，因为觉得瞬间传送这个主意很酷。但是故事进行到后半段的时候，就开始思考——瞬间传送究竟是一种移动的过程，还是一种死亡的过程？

而这一切，依然取决于“你”是什么。认同数据理论的人会认为到达纽约的你和从上海出发的你是相同的，瞬间传送并没有杀死你。但是我们都能理解故事结尾处那个还在上海的“你”的恐惧——人们是否能在确认自己的数据已经活在了纽约后，安然接受在上海被毁灭的现实。

更进一步说，如果传送器可以把你的数据送去纽约进行构造，那么它能不能把数据送去东京、伦敦和巴黎再造出三个同样的“你”？这时候要承认四个“你”全都是你就很难了吧？而传送机思想实验，其实就是对数据理论的有力的反驳。

我们从传送机思想实验中学到的，就是如果你的大脑数据被传送到了另一个大脑里面，哪怕这个大脑和你的大脑在分子结构上是相同的，这也只是创造了一个你的复制品而已——一个恰巧和你一模一样的陌生人。上海的那个你是特别的，当你被用另一些原子重造了之后，一些很重要的东西失去了，而这失去的东西才决定了什么是你。

这个失去的东西就是连续性。

年过90岁的祖父指着墙上的一张自己6岁时候的照片，说：“那个是我。”他当然没说错。但是，你要说照片里那个六岁小男孩和我面前这个九十多岁的老人是同一个人，难道不滑稽吗？这两个人没有任何的共同点。物理上来说，他们完全不一样，那个6岁小男孩身上的每一个细胞都死了几十年了。

至于他们的性格，我只能说这个小男孩大概是不会喜欢这个老爷子的。而且他们大脑里的数据几乎没有交集，街上随便找一个90岁的老人，他都比那个6岁的小男孩的数据更接近祖父。但是要记住，关键的不是相似性，而是连续性。如果相似性能够定义一个人的话，那上海的你和纽约的你就是同一个人了。我90岁的祖父和那个6岁小男孩所共有的是地球上任何一个人都不具备的，那就是这个90岁的老人可能不了解的自己6岁时候的样子，但是他记得自己89岁的样子，而那个89岁的人，记得自己85岁时候的样子。那个50岁的自己，记得自己43岁的样子。而那个7岁的自己，记得自己6岁时候的样子。这是一条长长的不断重叠的由记忆、性格和物理表征组成的链条。

就好像一艘年迈的特修斯之船，你可能已经修过它几百次，你一次又一次的替换它的木板，直到有一天你意识到这艘木船的每一片木板都被替换过了。这还是你的那艘小木船吗？是的，你知道，它还是特修斯之船。

也就是说，你不是一组数据，你是一个内容一直在变换的数据库，不断成长和更新。你不是一组原子，你是一套告诉这些原子该怎么组织的指令。<br>
你其实不是一个事物，而是一个故事，一个不断发展的主题。<br>
综上所述，人类未来命运最理想的图景即是：我们是这个浩瀚宇宙中第一批智能生物并且已经通过了大筛选，在不久的将来，创造出了保证安全性的超级人工智能服务于人类，使人类的生活质量大幅提高，以至于保持本我的，幸福的永生。<br>

但是，其中任何细小的差池，无论是遇到高等级的地外文明，还是不受控的人工智能，亦或是消灭本我的永生，都可以将人类从这个浩瀚宇宙中彻底地抹去。<br>
祝我们好运！

> 参考资料： <br>
The AI Revolution: Our Immortality or Extinction <br>
The AI Revolution: The Road to Superintelligence <br>
What Makes You You? <br>
The Fermi Paradox <br>
By Tim Urban <br>
感谢谢熊猫君在知乎上的中文翻译和罗辑思维的重新演绎。 <br>